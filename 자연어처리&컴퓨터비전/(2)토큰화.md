# 토큰화
* 자연어 처리(Natural Language Processing, NLP)는 컴퓨터가 인간의 언어를 이해하고 해석 및 생성하기 위한 기술을 의미한다.

* 인간의 언어를 이해하고 처리하기 위해 모호성(Ambiguity), 가변성(Variability), 구조(Structure) 문제를 해결해야하는데 이와 같은 문제를 해결하기 위해 **말뭉치**(Corpus)를 일정한 단위인 **토큰**(Token)으로 나눠야 한다.

* 말뭉치는 뉴스 기사, 사용자 리뷰 등 목적에 따라 구축되는 텍스트 데이터를 의미한다. 토큰은 개별 단어나 문장 부호와 같은 텍스트를 의미하며 말뭉치보다 더 작은 단위다.

* 토큰으로 나누는 과정은 **토큰화**(Tokenization)라고 하며 토큰화를 위해 **토크나이저**(Tokenizer)를 사용한다. 토크나이저란 텍스트 문자열을 토큰으로 나누는 알고리즘 또는 소프트웨어를 의미한다.

* 토크나이저를 구축하는 방법은 다음과 같다
    * 공백 분할: 텍스트를 공백 단위로 분리해 개별 단어로 토큰화한다.
    * 정규표현식 적용: 정규 표현식으로 특정 패턴을 식별해 텍스트를 분할한다.
    * 어휘 사전 적용: 사전에 정의된 단어 집합을 토큰으로 사용한다. 직접 어휘 사전을 구축하기 때문에 없는 단어나 토큰이 존재할 수 있다. 이러한 토큰을 **OOV**(Out Of Vocab)라고 한다.
    * 머신러닝 활용: 데이터세트를 기반으로 토큰화하는 방법을 학습한 머신러닝을 적용한다.


### 단어 및 글자 토큰화
1. **단어 토큰화(Word Tokenization)**
    * 텍스트 데이터를 의미 있는 단위인 단어로 분리하는 작업이다.

    * 단어 토큰화는 띄어쓰기, 문장 부호, 대소문자 등의 특정 구분자를 활용해 토큰화를 수행한다.
    ```
    review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
    tokenized = review.split()
 

    # 출력값
    ['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']
    ```

    * 단어 토큰화를 적용하면 '최고!'와 '최고!!'는 느낌표 하나의 차이지만 다른 토큰으로 나뉜다. 이처럼 단어 토큰화는 한국어 접사, 문장 부호, 오타 혹은 띄어쓰기 오류 등에 취약하다.

2. **글자 토큰화(Character Tokenization)**
    * 띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식으로, 비교적 작은 단어 사전을 구축할 수 있다는 장점이 있다.
    ```
    review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
    tokenized = list(review)

    
    # 출력값
    ['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']
   ```

    * 영어의 경우 글자 토큰화를 진행하면 각 알파벳으로 나뉜다. 하지만 한글의 경우 하나의 글자는 여러 자음과 모음으로 이루어져 있다. 그러므로 자소 단위로 나눠서 자소 단위 토큰화를 진행한다.

    * 자모 변환 함수는 입력된 한글 문자열을 조합형 한글 자모로 변환하는 함수다. **조합형**은 글자를 자모 단위로 나눠 인코딩한 뒤 이를 조합해 한글을 표현한다. **완성형**은 조합된 글자 자체에 값을 부여해 인코딩하는 방식이다.
    ```
    # 완성형으로 입력된 한글을 조합형 한글로 변환
    retval = jamo.h2j(
        hangul_string
    )

    # 조합형 한글 문자열을 자소 단위로 나눠 반환하는 함수
    retval = jamo.j2hcj(
        jamo
    )
    ```
    ```
    from jamo import h2j, j2hcj

    review = "현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!"
    decomposed = j2hcj(h2j(review))
    tokenized = list(decomposed)


    # 출력값
    ['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']
    ```

### 형태소 토큰화(Morpheme Tokenization)
* 텍스트를 형태소 단위로 나누는 토큰화 방법으로 언어의 문법과 구조를 고려해 단어를 분리하고 이를 의미 있는 단위로 분류하는 작업이다.

* '그', '-는', '나', '-에게' 등과 같이 실제로 의미를 가지고 있는 최소의 단위를 **형태소**(Morpheme)라고 한다.

* 형태소는 스스로 의미를 가지고 있는 **자립 형태소**(Free Morpheme)와 스스로 의미를 갖지 못하고 다른 형태소와 조합되어 사용되는 **의존 형태소**(Bound Morpheme)로 구분된다.

1. **형태소 어휘 사전(Morpheme Vocabulary)**
    * 자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전을 의미한다.

    * 일반적으로 형태소 어휘 사전에는 각 형태소가 어떤 품사에 속하는지와 해당 품사의 뜻 등의 정보도 함께 제공된다.

    * 텍스트 데이터를 형태소 분석하여 각 형태소에 해당하는 **품사**(Part Of Speech, POS)를 태깅하는 작업을 **품사 태깅**(POS Tagging)이라고 한다. 이를 통해 자연어 처리 분야에서 문맥을 고려할 수 있다.

2. **KoNLPy**
    * 라이브러리로 명사 추출, 형태소 분석, 품사 태깅 등의 기능을 제공한다.

    * 라이브러리 설치
    ```
    pip install konlpy
    ```
    ```
    # Ojt 토큰화
    from konlpy.tag import Okt

    okt = Okt()

    sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

    nouns = okt.nouns(sentence) # 명사
    phrases = okt.phrases(sentence) # 구
    morphs = okt.morphs(sentence) # 형태소
    pos = okt.pos(sentence) # 품사 태깅


    # 출력값
    명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']
    구 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']
    형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']
    품사 태깅 : [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]

    ```
    ```
    # 꼬꼬마 토큰화
    from konlpy.tag import Kkma

    kkma = Kkma()

    sentence = "무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다."

    nouns = kkma.nouns(sentence) # 명사
    sentences = kkma.sentences(sentence) # 구문
    morphs = kkma.morphs(sentence) # 형태소
    pos = kkma.pos(sentence) # 품사 태깅


    # 출력값
    명사 추출 : ['무엇', '상상', '수', '사람', '무엇']
    구문 추출 : ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']
    형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']
    품사 태깅 : [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]

    ```

3. **NLTK(Natural Language Toolkit)**
    * 라이브러리로 토큰화, 형태소 분석, 구문 분석, 개체명 인식, 감성 분석 등과 같은 기능을 제공한다.

    * 라이브러리 설치
    ```
    pip install nltk
    ```
    ```
    # punk 모델 기반으로 영문 토큰화
    import nltk
    from nltk import tokenize

    nltk.download("punkt") # 토큰화 작업을 위한 모델 다운
    nltk.download("averaged_perceptron_tagger") # 품사 태깅 작업을 위한 모델 다운

    sentence = "Those who can imagine anything, can create the impossible."

    word_tokens = tokenize.word_tokenize(sentence) # 공백 기준 단어 분리
    sent_tokens = tokenize.sent_tokenize(sentence) # 구두점을 기준으로 문장 분리

    # 출력값
    ['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']
    ['Those who can imagine anything, can create the impossible.']
    ```
    ```
    # Averaged Perceptron Tagger 모델 기반으로 영문 품사 태깅
    from nltk import tag
    from nltk import tokenize

    sentence = "Those who can imagine anything, can create the impossible."

    word_token = tokenize.word_tokenize(sentence)
    pos = tag.pos_tag(word_token)

    print(pos)


    # 출력값
    [('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]
    ```

4. **spaCy**
    * NLTK 라이브러리와 유사하지만 주요한 차이점은 빠른 속도와 높은 정확도를 목표하는 머신러닝 기반의 자연어 처리 라이브러리이다. NLTK에서 사용하는 모델보다 더 크고 복잡하며 더 많은 리소스를 요구한다.

    * spacy는 사전 학습된 모델을 제공한다. 영어로 사전 학습된 모델인 en_core_web_sm을 설치한다.

    * spaCy 설치
    ```
    pip install spacy
    python -m spacy download en_core_web_sm
    ```
    ```
    # 품사 태깅
    import spacy

    nlp = spacy.load('en_core_web_sm')
    sentence = "Those who can imagine anything, can create the impossible."
    doc = nlp(sentence) # doc 객체 반환

    for token in doc:   # doc 객체는 여러 token 객체로 이루어져 있다.
        print(f"[{token.pos_:5} - {token.tag_:3}] : {token.text}")
    # pos_ 속성: 기본 품사 속성, tag_ 속성: 세분화 품사 속성

    
    # 출력값
    [PRON  - DT ] : Those
    [PRON  - WP ] : who
    [AUX   - MD ] : can
    [VERB  - VB ] : imagine
    [PRON  - NN ] : anything
    [PUNCT - ,  ] : ,
    [AUX   - MD ] : can
    [VERB  - VB ] : create
    [DET   - DT ] : the
    [ADJ   - JJ ] : impossible
    [PUNCT - .  ] : .
    ```



### 하위 단어 토큰화(Subword Tokenization)
* 형태소 분석기는 전문용어, 고유어, 신조어, 맞춤법이나 띄어쓰기가 지켜지지 않은 경우에 취약점을 보인다.

* 현대 자연어 처리에서는 신조어 발생, 오탈자, 축약어 등을 고려해야 하기에 분석할 단어의 양이 많아져 어려움을 겪는다. 이를 해결하기 위한 방법 중 하나로 **하위 단어 토큰화**가 있다.

* 하위 단어 토큰화란 하나의 단어가 빈번하게 사용되는 **하위 단어**(Subword)의 조합으로 나누어 토큰화하는 방법이다.

1. **바이트 페어 인코딩(Byte Pair Encoding, BPE)**
    * 다이어그램 코딩(Diagram Coding)이라고도 하며 하위 단어 토큰화의 한 종류다. 텍스트 데이터에서 가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘이다.

    * 이 알고리즘은 연속된 글자 쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 조합탐지와 부호화를 반복하며 이 과정에서 자주 등장하는 단어는 하나의 토큰으로 토큰화되고, 덜 등장하는 단어는 토큰의 조합으로 표현된다.

    * **센텐스피스(Sentencepiece)** 와 **코포라(Korpora)** 라이브러리 설치
    ```
    pip install sentencepiece Korpora
    ```

    * 바이트 페어 인코딩을 수행하는 토크나이저 모델 학습
    ```
    # 청와대 청원 데이터 다운로드
    from Korpora import Korpora

    corpus = Korpora.load("korean_petitions") # 말뭉치 불러오기
    dataset = corpus.train
    petition = dataset[0] # 첫 번째 청원 데이터

    print("청원 시작일 :", petition.begin)
    print("청원 종료일 :", petition.end)
    print("청원 동의 수 :", petition.num_agree)
    print("청원 범주 :", petition.category)
    print("청원 제목 :", petition.title)
    print("청원 본문 :", petition.text[:30])


    # 출력값
    청원 시작일 : 2017-08-25
    청원 종료일 : 2017-09-24
    청원 동의 수 : 88
    청원 범주 : 육아/교육
    청원 제목 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.
    청원 본문 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비
    ```
    ```
    # 학습 데이터세트 생성
    petitions = corpus.get_all_texts()
    with open("./datasets/corpus.txt", "w", encoding="utf-8") as f:
    for petition in petitions:
        f.write(petition + '\n')
    ```
    ```
    # 토크나이저 모델 학습
    from sentencepiece import SentencePieceTrainer

    SentencePieceTrainer.Train(
        "--input=../datasets/corpus.txt\    # 말뭉치 텍스트 파일 경로
        --model_prefix=petition_bpe\        # 모델 파일 이름
        --vocab_size=8000 model_type=bpe"   # 어휘 사전 크기, 토크나이저 알고리즘
    )
    ```
    ```
    # 바이트 페어 인코딩 토큰화
    from sentencepiece import SentencePieceProcessor

    tokenizer = SentencePieceProcessor()
    tokenizer.load("../models/petition_bpe.model")

    sentence = "안녕하세요, 토크나이저가 잘 학습되었군요!"
    sentences = ["이렇게 입력값을 리스트로 받아서", "쉽게 토크나이저를 사용할 수 있답니다"]

    tokenized_sentence = tokenizer.encode_as_pieces(sentence) # 문장을 토큰화
    tokenized_sentences = tokenizer.encode_as_pieces(sentences)
    print("단일 문장 토큰화 :", tokenized_sentence)
    print("여러 문장 토큰화 :", tokenized_sentences)

    encoded_sentence = tokenizer.encode_as_ids(sentence) # 토큰을 정수로 인코딩
    encoded_sentences = tokenizer.encode_as_ids(sentences)
    print("단일 문장 정수 인코딩 :", encoded_sentence)
    print("여러 문장 정수 인코딩 :", encoded_sentences)

    decode_ids = tokenizer.decode_ids(encoded_sentences) # 정수를 다시 문자열 데이터로 변환
    decode_pieces = tokenizer.decode_pieces(encoded_sentences)
    print("정수 인코딩에서 문장 변환 :", decode_ids)
    print("하위 단어 토큰에서 문장 변환 :", decode_pieces)
    ```
    ```
    # 어휘 사전 불러오기
    tokenizer = SentencePieceProcessor()
    tokenizer.load("petition_bpe.model")

    vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}
    print(list(vocab.items())[:5])
    print("vocab size :", len(vocab))
    ```



2. **워드피스(Wordpiece)**
 * 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합한다. 모델이 새로운 하위 단어를 생성할 때 이전 하위 단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어를 선택한다.

 * 각 글자 쌍에 대한 점수 수식
    $$score = \frac{f(x,y)}{f(x),f(y)}$$
    * f는 빈도(frequency)를 나타내는 함수이며, x와 y는 병합하려는 하위 단어를 의미한다. 그러므로 f(x,y)는 x와 y가 조합된 글자 쌍의 빈도를 의미한다.

* 허깅 페이스 토크나이저스 라이브러리 설치
    * 토크나이저스 라이브러리는 **정규화**(Normalization)와 **사전 토큰화**(Pre-tokenization)을 제공한다.
```
pip install tokenizers
```     

* 정규화는 일관된 형식으로 텍스트를 표준화하고 모호한 경우를 방지하기 위해 일부 문자를 대체하거나 제거하는 등의 작업을 수행한다.

* 사전 토큰화는 입력 문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능을 제공한다.
```
# 워드피스 토크나이저 학습
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.normalizers import Sequence, NFD, Lowercase
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(WordPiece()) # 워드피스 모델 불러오기

tokenizer.normalizer = Sequence([NFD(), Lowercase()]) # 정규화 방식 설정
tokenizer.pre_tokenizer = Whitespace() # 사전 토큰화 방식 설정

tokenizer.train(["../datasets/corpus.txt"])
tokenizer.save("../models/pertition_wordpiece.json") # 학습 결과 저장
```

* 워드 피스 모델의 학습이 완료되면 petition_wordpiece.json 파일이 생성된다. json 파일을 이용해 토크나이저를 수행해 보자
```
# 워드피스 토큰화
from tokenizers import Tokenizer
from tokenizers.decoders import WordPiece as WordPieceDecoder

tokenizer = Tokenizer.from_file("../models/petition_wordpiece.json") # 모델을 불러와 Tokenizer 객체 생성
tokenizer.decoder = WordPieceDecoder() # Tokenizer 디코더를 워드피스 디코더로 설정

sentence = "안녕하세요, 토크나이저가 잘 학습되었군요!"
sentences = ["이렇게 입력값을 리스트로 받아서", "쉽게 토크나이저를 사용할 수 있답니다"]

# encode 메서드로 문장을 토큰화, 각 토큰의 색인 번호 반환
encoded_sentence = tokenizer.encode(sentence)
# encode_batch 메서드로 여러 문장을 한 번에 토큰화, 각 토큰의 색인 번호 반환
encoded_sentences = tokenizer.encode_batch(sentences)


print("인코더 형식 :", type(encoded_sentece))

# 토큰화된 데이터는 tokens 속성을 통해 확인 가능
print("단일 문장 토큰화 :", encoded_sentence.tokens)
print("여러 문장 토큰화 :", [enc.tokens for enc in encoded_sentences])

# 토큰 정수(ids) 속성으로 인코딩된 문장의 ID 값을 출력
print("단일 문장 정수 인코딩 :", encoded_sentence.ids)
print("여러 문장 정수 인코딩 :", [enc.ids for enc in encoded_sentences])

# decode 메서드를 통해 정수 인코딩된 결과를 다시 문장으로 디코딩
print("정수 인코딩에서 문장 변환 :", tokenizer.decode(encoded_sentence.ids))
```