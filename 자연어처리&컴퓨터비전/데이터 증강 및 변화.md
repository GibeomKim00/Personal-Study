# 데이터 증강 및 변화
* **데이터 증강**(Data Augmentation)이란 데이터가 가진 고유한 특징을 유지한 채 변형하거나 노이즈를 추가해 데이터세트의 크기를 늘리는 방법이다.

* 데이터 증강은 모델의 과대적합을 줄이고 일반화 능력을 향상시킬 수 있다. 그러나 기존 데이터를 너무 많이 변형하거나 노이즈를 추가한다면 기존 데이터가 가진 특징이 파괴될 수 있다.

### 텍스트 데이터
* 텍스트 데이터 증강 방법은 크게 삽입, 삭제, 교체, 대체, 생성, 반의어, 맞춤법 교정, 역번역 등이 있다.

* 설치해야 할 라이브러리
```
pip install numpy requests nlpaug transformers sacremoses nltk
```

1. **삽입 및 삭제**
    * **삽입**은 의미 없는 문자나 단어, 또는 문장 의미에 영향을 끼치지 않는 수식어 등을 추가하는 방법이다.

    * **삭제**는  삽입과 반대로 임의의 단어나 문자를 삭제해 데이터의 특징을 유지하는 방법이다.

    * 너무 적은 양을 삽입 및 삭제를 할 경우 과대적합이 발생할 수 있고, 반대로 너무 많은 양을 삽입 및 삭제를 할 경우 데이터 품질 저하로 이어질 수 있다.

    * **ContextualWordEmbsAug** 클래스는 BERT 모델을 활용해 단어를 삽입하는 기능을 제공한다. 해당 클래스는 대체(substitute) 기능도 제공한다.
    ```
    import nlpaug.augmenter.word as naw

    texts = [
        "Those who can imagine anything, can create the impossible.",
        "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
        "If a machine is expected to be infallibale, it cannot also be intelligent"
    ]

    aug = naw.ContextualWordEmbsAug(model_path="bert-base-uncased", action="insert")
    augmented_texts = aug.augment(texts)

    for text, augmented in zip(texts, augmented_texts):
    print(f"src : {text}")
    print(f"dst : {augmented}")
    print("-----------------")


    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : those women who cannot can imagine anything, can not create but the impossible.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : we can only see a short distance way ahead, maybe but we ultimately can just see plenty back there so that also needs to be done.
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : if is a turing machine which is expected now to be less infallibale, it cannot also possibly be intelligent
    -----------------
    ```

    * **RandomCharAug** 클래스를 활용한 문자 삭제. 해당 클래스는 삽입(insert), 대체(substitute), 교체(swap), 삭제(delete) 기능을 제공한다.
    ```
    import nlpaug.augmenter.char as nac

    texts = [
        "Those who can imagine anything, can create the impossible.",
        "We can only see a short distance ahead, but we can see plenty there that needs to be done.",
        "If a machine is expected to be infallibale, it cannot also be intelligent"
    ]

    aug = nac.RandomCharAug(action="delete")
    augmented_texts = aug.augment(texts)

    for text, augmented in zip(texts, augmented_texts):
    print(f"src : {text}")
    print(f"dst : {augmented}")
    print("-----------------")

    
    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : Those who can imie nting, can rate the iossibl.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : We can only see a hot stace aad, but we can see pley hre at eds to be done.
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : If a mahe is eeced to be nfaible, it cannot ao be iellgnt
    -----------------
    ```

2. **교체 및 대체**
    * **교체**는 단어나 문자의 위치를 교환하는 방법이다. 교체는 무의미하거나 의미상 잘못된 문장을 생성할 수 있으므로 데이터의 특성에 따라 주의해 사용해야 한다.

    * **대체**는 단어나 문자를 임의의 단어나 문자로 바꾸거나 동의어로 변경하는 방법을 의미한다.

    * **RandomWordAug** 클래스를 이용한 단어 교체, RandomWordAug 클래스는 삽입, 대체, 교체, 삭제 기능을 제공하며, 자르기(crop) 기능도 지원한다.
    ```
    aug = naw.RandomWordAug(action="swap")
    augmented_texts = aug.augment(texts)


    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : Who can those imagine anything can, the create impossible.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : We can see a only short distance ahead, but see we can plenty there needs that be to. done
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : A if machine is to expected be infallibale it, cannot be intelligent also
    -----------------
    ```

    * **SynonymAug** 클래스를 이용한 단어 대체, 워드넷(WordNet) 데이터베이스나 의역 데이터 베이스(The Parapharse Database, PPDB)를 활용해 단어를 대체한다.

    * 해당 기능은 문맥을 파악해 동의어를 변경하는 것이 아닌 데이터베이스 내 유의어나 동의어로 변경하므로 본래의 문맥과 전혀 다른 문장이 생성될 수 있다.
    ```
    aug = naw.SynonymAug(aug_src="wordnet")
    augmented_texts = aug.augment(texts)


    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : Those world health organization can imagine anything, can produce the insufferable.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : We can entirely see a forgetful distance ahead, simply we can see enough there that need to be done.
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : If a machine personify expected to cost infallibale, information technology cannot likewise represent intelligent
    -----------------
    ```

    * **ReservedAug** 클래스를 이용한 단어 대체, ReservedAug 클래스는 입력 데이터에 포함된 단어를 특정한 단어(reserved_tokens)로 대체하는 기능을 제공한다.
    ```
    reserved_tokens = [
        ["can", "can't", "connot", "could"]
    ]

    reserved_aug = naw.ReservedAug(reserved_tokens=reserved_tokens)
    augmented_texts = reserved_aug.augment(texts)


    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : Those who connot imagine anything, connot create the impossible.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : We connot only see a short distance ahead, but we can't see plenty there that needs to be done.
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : If a machine is expected to be infallibale, it cannot also be intelligent
    -----------------
    ```

3. **역번역**
    * 역번역(Back-translation)이란 입력 텍스트를 특정 언어로 번역한 다음 다시 본래의 언어로 번역하는 방법을 의미한다.

    * 역번역은 번역 모델의 성능에 따라 결과가 크게 달라질 수 있다.

    * **BackTranslationAug** 클래스를 이용한 역번역
    ```
    back_translation = naw.BackTranslationAug(
        from_model_name="facebook/wmt19-en-de",
        to_model_name="facebook/wmt19-de-en"
    )
    augmented_texts = back_translation.augment(texts)


    # 출력값
    src : Those who can imagine anything, can create the impossible.
    dst : Anyone who can imagine anything can achieve the impossible.
    -----------------
    src : We can only see a short distance ahead, but we can see plenty there that needs to be done.
    dst : We can only look a little ahead, but we can see a lot there that needs to be done.
    -----------------
    src : If a machine is expected to be infallibale, it cannot also be intelligent
    dst : If a machine is expected to be infallible, it cannot be intelligent either
    -----------------
    ```


### 이미지 데이터
* 이미지 데이터 증강 방법은 크게 회전, 대칭, 이동, 크기 조정 등이 있다.

* 이미지 데이터 증강 방법은 토치비전 라이브러리의 **transform** 모듈을 통해 수행할 수 있다. 또한 여러 모델 매개변수를 묶어주는 Sequential과 같은 역할을 하는 **Compose** 클래스를 함께 사용해 증강을 적용한다.

* **ToTensor**() 클래스는 PIL.Image 형식을 Tensor 형식으로 변환한다. ToTensor 클래스는 [0~255] 범위의 픽셀값을 [0.0~1.0] 사이의 값으로 **최대 최소 정규화(Min-max Normalization)** 를 수행한다. 또한 입력 데이터의 [높이, 너비, 채널] 형태를 [채널, 높이, 너비] 형태로 변환한다.
```
from PIL import Image
from torchvision import transforms

transform = transforms.Compose(
    [
        transforms.Resize(size=(512,512)),
        transforms.ToTensor()
    ]
)

image = Image.open('../datasets/images/cat.jpg')
transformed_image = transform(image)
```

1. **회전 및 대칭**
    * -30~30도 사이로 회전시키면서 수평 대칭과 수직 대칭을 50% 확률로 적용하는 코드이다.
    * 회전을 하면서 이미지 여백이 생길 수 있는데 expand 값을 참으로 하면 여백이 채워진다. center(중심점)는 sequence 형태로 전달하며 입력하지 않으면 왼쪽 상단을 기준으로 회전한다.
    ```
    transform = transforms.Compose(
        [
            transforms.RandomRotation(degrees=30, expand=False, center=None),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5)
        ]
    )
    ```

2. **자르기 및 패딩**
    * RandomCrop 클래스는 정수나 시퀀스 형태로 값을 입력할 수 있다. 정수로 입력한다면 이미지의 높이와 너비가 동일한 정사각형 이미지로 잘리며, 시퀀스로 입력하는 경우 (높이, 너비) 순서로 이미지를 자른다.

    * Pad 클래스는 이미지를 512 X 512 크기로 자른 다음 50의 패딩을 주었다. 패딩은 모든 방향으로 적용되므로 612 X 612 크기의 이미지로 반환된다. 패딩 방식(padding_mode)은 상수(constant)로 입력해 RGB(127,127,125)로 테두리가 생성된다.

    * 패딩 방식을 반사(reflect)나 대칭(symmetric)으로 준다면 입력한 RGB는 무시되며 이미지의 픽셀값을 반사하거나 대칭해 생성한다.
    ```
    transform = transforms.Compose(
        [
            transforms.RandomCrop(size=(512,512)),
            transforms.Pad(padding=50, fill=(127,127,255), padding_mode="constant"),
        ]
    )
    ```

3. **크기 조정**
    * 이미지 처리 모델 학습을 원활하게 진행하기 위해서는 학습 데이터에 사용되는 이미지의 크기가 모두 일정해야 한다.

    * Resize 클래스도 크기(size) 매개변수를 정수 또는 시퀀스로 입력받는다. 정수로 입력받는 경우 높이나 너비 중 크기가 더 작은 값에 비율을 맞춰 크기가 수정된다.
    ```
    transform = transforms.Compose(
        [
            transforms.Resize(size=(512,512))
        ]
    )
    ```

4. **변형**
    * 이미지를 변환하는 경우 **기하학적 변환**(Geometric Transform)을 통해 이미지를 변경한다. 기하학적 변환이란 인위적으로 확대, 축소, 위치 변경, 회전, 왜곡하는 등 이미지의 형태를 변환하는 것을 의미하며 크게 아핀 변환과 원근 변환이 있다.
        * 아핀 변환(Affine Transformation): 2 X 3 행렬을 사용하며 행렬 곱셈에 벡터 합을 활용해 표현할 수 있는 변환을 의미한다.
        * 원근 변환(Perspective Transformation): 3 X 3 행렬을 사용하며, 호모그래피(Homography)로 모델링할 수 있는 변환을 의미한다.

    * 아핀 변환은 각도(degrees), 이동(translate), 척도(scale), 전단(shear)을 입력해 이미지를 변형한다.

    * 이미지의 축을 비트는 것처럼 변환되므로 특징들을 유지하지만, 이미지 픽셀들이 큰 폭으로 변환되므로 가장 많은 변형이 일어난다.
    ```
    transform = transforms.Compose(
        [
        transforms.RandomAffine(
            degrees=15, translate=(0.2, 0.2),
            scale=(0.8, 1.2), shear=15
            )
        ]
    )
    ```

5. **색상 변환**
    * 모델이 이미지를 분석할 때 특정 색상에 편향되지 않도록 픽셀값을 변환하거나 정규화하면 모델을 더 일반화해 분석 성능을 향상시키고 학습을 단축시킬 수 있다.

    * 색상 변환 클래스(ColorJitter)는 이미지의 밝기(brightness), 대비(contrast), 채도(saturation), 색상(hue)을 변환한다.

    * 정규화 클래스(Normalize)는 픽셀의 평균과 표준편차를 활용해 정규화한다.
    ```
    transform = transforms.Compose(
        [
            transforms.ColorJitter(
                brightness=0.3, contrast=0.3,
                saturation=0.3, hue=0.3
            ),
            transforms.ToTensor(),
            transforms.Normalize(
                mean = [0.485, 0.456, 0.406],
                std = [0.229, 0.224, 0.225],
            ),
            transforms.ToPILImage()
        ]
    )
    ```

6. **노이즈**
    * 노이즈 추가도 특정 픽셀값에 편향되지 않도록 임의의 노이즈를 추가해 모델의 일반화 능력을 높이는 데 사용된다.
    ```
    import numpy as np
    from imgaug import augmenters as iaa

    class IaaTransforms:
        def __init__(self):
            self.seq = iaa.Sequential([
                iaa.SaltAndPepper(p=(0.03, 0.07)),
                iaa.Rain(speed=(0.3, 0.7))
            ])
        
        def __call__(self, images):
            images = np.array(images)
            augmented = self.seq.augment_image(images)
            return Image.fromarray(augmented)

        transform = transforms.Compose(
            [
                IaaTransforms()
            ]
        )     
    ```

7. **컷아웃 및 무작위 지우기**  
    * **컷아웃**(Cutout)은 이미지에서 임의의 사각형 영역을 삭제하고 0의 픽셀값으로 채우는 방법이며, **무작위 지우기**(Random Erasing)는 임의의 사각형 영역을 삭제하고 무작위 픽셀값으로 채우는 방법이다.

    * 두 가지 방법 모두 이미지의 객체가 일부 누락되더라고 모델을 견고하게 만드는 증강 방법이다.

    * 무작위 지우기 클래스(RandomErasing)의 값(value)을 0으로 할당하면 컷아웃 방법이 되며, random으로 입력하면 무작위 지우기 방법이 된다. 단, 무작위 지우기 클래스는 Tensor 형식만 지원되므로 Tensor 형식으로 변환해야 한다.
    ```
    transform = transforms.Compose(
        [
            transforms.ToTensor(),
            transforms.RandomErasing(p=1.0, value=0),
            transforms.RandomErasing(p=1.0, value="random"),
            transforms.ToPILImage(),   
        ]
    )
    ```

8. **혼합 및 컷믹스**
    * **혼합**(Mixup)은 두 개 이상의 이미지를 혼합해 새로운 이미지를 생성하는 방법이다. 생성된 이미지는 두 개의 이미지가 겹쳐 흐릿한 형상을 지니게 된다.

    * **컷믹스**(CutMix)는 이미지 패치(patch) 영역에 다른 이미지를 덮어씌우는 방법이다. 모델이 이미지의 특정 영역을 기억해 인식하는 문제를 완화하며, 이미지 전체를 보고 판단할 수 있게 일반화한다.

    * alpha와 beta는 각 이미지의 혼합 비율을 설정하고 scale을 통해 이미지 크기를 조절한다.

    ```
    # 혼합
    class Mixup:
    def __init__(self, target, scale, alpha=0.5, beta=0.5):
        self.target = target
        self.scale = scale
        self.alpha = alpha
        self.beta = beta

    def __call__(self, image):
        image = np.array(image)
        target = self.target.resize(self.scale)
        target = np.array(target)
        mix_image = image * self.alpha + target * self.beta
        return Image.fromarray(mix_image.astype(np.uint8))

    transform = transforms.Compose(
        [
            transforms.Resize((512,512)),
            Mixup(
                target=Image.open('/content/drive/MyDrive/dog.jpg'),
                scale=(512,512),
                alpha=0.5,
                beta=0.5
            )   
        ]
    )
    ```


### 사전 학습된 모델(Pre-trained Model)
* 대규모 데이터세트로 학습된 딥러닝 모델로 이미 학습이 완료된 모델을 의미한다.

* 사전 학습된 모델은 **전이 학습**(Transfer Learning)과 같은 작업뿐만 아니라 **백본 네티워크**(Backbone Networks)로 사용된다.

1. **백본**
    * 입력 데이터에서 특징을 추출해 최종 분류기에 전달하는 딥러닝 모델이나 딥러닝 모델의 일부를 의미한다.

    * 백본 네트워크는 입력 데이터에서 특징을 추출하므로 노이즈와 불필요한 특성을 제거하고 가장 중요한 특징을 추출할 수 있다.


2. **전이 학습**
    * 어떤 작업을 수행하기 위해 이미 사전 학습된 모델을 재사용해 새로운 작업이나 관련 도메인의 성능을 향상시킬 수 있는 기술을 의미한다.

    * 기존 머신러닝 모델이 학습했던 개, 고양이의 특징과 유사한 동물 특징 영역(눈, 코, 입)을 학습하여 **소스 도메인**(Source Domain)에서 학습한 지식을 활용해 **타깃 도메인**(Target Domain)에서 모델의 성능을 향상시키는 것이다.
    
    * 전이 학습을 수행하기 위해 사전 학습된 모델을 **업스트림**(Upstream) 모델이라고 하며, 미세 조정된 모델은 **다운스트림**(Downstream) 모델이라고 한다.

    * 전이 학습에는 **귀납적 전이 학습**, **변환적 전이 학습**, **비지도 전이 학습** 등이 있다.
        * 귀납적 전이 학습(Inductive Transfer Learning): 기존에 학습한 모델의 지식을 활용하여 새로운 작업을 수행하기 위한 방법 중 하나다. 귀납적 전이 학습은 **자기주도적 학습**(Self-taught Learning)과 **다중 작업 학습**(Multi-task Learning)으로 나뉜다.
            * 자기주도적 학습이란 비지도 전이 학습의 유형 중 하나로, 소스 도메인의 데이터세트에서 데이터의 양은 많으나 레이블링된 데이터의 수가 매우 적거나 없을 때 사용하는 방법이다. 레이블이 지정되지 않은 대규모 데이터세트에서 특징을 추출하는 오토 인코더와 같은 모델을 학습시킨 다음, 저차원 공간에서 레이블링된 데이터로 미세 조정하는 방법을 의미한다.
            * 다중 작업 학습은 레이블이 지정된 소스 도메인과 타깃 도메인 데이터를 기반으로 모델에 여러 작업을 동시에 가르치는 방법을 의미한다. 다중 작업 학습의 모델 구조는 **공유 계층**(Shared Layer)과 **작업별 계층**(Task Specific Layer)으로 나뉜다.
        
        * 변환적 전이 학습(Transductive Transfer Learning): 소스 도메인과 타깃 도메인이 유사하지만 완전히 동일하지 않은 경우를 의미한다. 변화적 전이 학습에 사용되는 소스 도메인은 레이블이 존재하며, 타깃 도메인에는 레이블이 존재하지 않은 경우에 사용된다. 변환적 전이 학습은 **도메인 적응**(Domain Adaptation)과 **표본 선택 편향/공변량 이동**(Sample Selection Bias/Covariance Shift)으로 나뉜다.
            * 도메인 적응이란 소스 도메인과 타깃 도메인의 특징 분포를 전이시키는 방법이다. 서로 다른 도메인들의 특징 분포를 고려해 학습하므로 도메인 변화를 확인해 전이하게 된다.
            * 표본 선택 편향/공변량 이동이란 소스 도메인과 타깃 도메인의 분산과 편향이 크게 다를 때 표본을 선택해 편향이나 공변량을 이동시키는 방법을 의미한다.

        * 비지도 전이 학습(Unsupervised Transfer Learning): 소스 도메인과 타깃 도메인 모두 레이블이 지정된 데이터가 없는 전이 학습 방법이다. 레이블이 없는 전체 데이터로 학습해 데이터가 가진 특징과 특성을 구분할 수 있게 사전 학습된 모델을 구축하고 소규모의 레이블이 지정된 데이터를 활용해 미세 조정한다.

        * 제로-샷 전이 학습(Zero-shot Transfer Learning): 사전에 학습된 모델을 이용해 다른 도메인에서도 적용할 수 있는 전이 학습 기법 중 하나다. 이를 통해 새로운 도메인에서 일반화된 성능을 가질 수 있다.

        * 원-샷 전이 학습(One-shot Transfer Learning): 제로-샷 학습과 유사하지만, 한 번에 하나의 샘플만 사용해 모델을 학습하는 방법이다. 따라서 매우 적은 양의 데이터를 이용하여 분류 문제를 해결할 수 있다. 원-샷 전이 학습 모델은 **서포트 셋**(Support Set)과 **쿼리 셋**(Query Set)을 가정한다. 마지막으로, 서포트 셋에 있는 대표 샘플과 쿼리 셋 간의 거리를 측정하여 쿼리 셋과 가장 가까운 서포트 셋의 대표 샘플의 클래스로 분류한다.
            * 서포트 셋은 학습에 사용될 클래스의 대표 샘플을 의미한다.
            * 쿼리 셋은 새로운 클래스를 분류하기 위한 입력 데이터를 의미하며, 분류 대상 데이터로, 서포트 셋에서 수집한 샘플과는 다른 샘플이어야 한다.


### 특징 추출 및 미세 조정
* 특징 추출 및 미세 조정은 전이 학습에 사용되는 일반적인 기술들로 두 가지 모두 대규모 데이터세트로 사전 학습된 모델을 작은 데이터세트로 추가 학습해 가중치나 편향을 수정한다.

* **특징 추출**(Feature Extraction)은 타깃 도메인이 소스 도메인과 유사하고 타깃 도메인의 데이터세트가 적을 때 사용된다. 두 도메인이 유사하기에 가중치와 편향도 유사하다. 따라서 특징 추출 계층은 동결해 학습하지 않고 기존에 학습된 모델의 가중치를 사용한다.

* **미세 조정**(Fine-tunning)은 특징 추출 계층을 일부만 동결하거나 동겨하지 않고 타깃 도메인에 대한 학습을 진행한다.